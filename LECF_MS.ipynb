{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imports\n",
    "\"\"\"\n",
    "# import mindspore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore import nn\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore.common.parameter import ParameterTuple\n",
    "from mindspore.ops import composite as C\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore import Tensor\n",
    "from mindspore.nn.layer.activation import get_activation\n",
    "import mindspore.context as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load data\n",
    "\"\"\"\n",
    "class Data(object):\n",
    "    def __init__(self,npzpath=\"./data/viedo10/video10.npz\"):\n",
    "\n",
    "        self.user_item = defaultdict(set)\n",
    "        self.item_user = defaultdict(set)\n",
    "\n",
    "        self.user_vali_item = dict()\n",
    "        self.user_test_item = dict()\n",
    "\n",
    "        _data = np.load(npzpath, allow_pickle=True)\n",
    "        self.train_data = _data['train_data']\n",
    "        self.test_data = _data['test_data'].tolist()\n",
    "        vali_data = _data['vali_data'].tolist()\n",
    "\n",
    "        # todo consider using os.path.join\n",
    "        p = npzpath.split('/')\n",
    "        self.path = p[0] + '/' + p[1] + '/' + p[2]\n",
    "\n",
    "        self.n_users, self.n_items = self.train_data.max(axis=0) + 1\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        for u, i in self.train_data:\n",
    "            self.user_item[u].add(i)\n",
    "            self.item_user[i].add(u)\n",
    "\n",
    "            self.R[u, i] = 1.\n",
    "\n",
    "        self.train_number = np.shape(self.train_data)[0]\n",
    "        print(self.n_users, self.n_items,self.train_number, self.train_number/(self.n_users*self.n_items))\n",
    "\n",
    "        for u in self.test_data.keys():\n",
    "            self.user_test_item[u]=[self.test_data[u][0]]\n",
    "            self.user_test_item[u].extend(self.test_data[u][1])\n",
    "\n",
    "        for u in vali_data.keys():\n",
    "            self.user_vali_item[u] = [vali_data[u][0]]\n",
    "            self.user_vali_item[u].extend(vali_data[u][1])\n",
    "\n",
    "        # self.nodesum = self.get_nodesum(depth)\n",
    "    def gen_batch_train_data(self, neg_number, batch_size):\n",
    "        np.random.shuffle(self.train_data)\n",
    "        batch = np.zeros((batch_size, 3), dtype=np.uint32)\n",
    "        idx = 0\n",
    "        for u,i in self.train_data:\n",
    "            for neg_num in range(neg_number):\n",
    "                neg_item = random.randint(0, self.n_items - 1)\n",
    "                while (neg_item in self.user_item[u]):\n",
    "                    neg_item = random.randint(0, self.n_items  - 1)\n",
    "                batch[idx, :] = [u,i, neg_item]\n",
    "                idx += 1\n",
    "\n",
    "                if (idx == batch_size):\n",
    "                    yield batch\n",
    "                    idx = 0\n",
    "\n",
    "        if (idx > 0):\n",
    "            yield batch[:idx]\n",
    "    def gen_batch_test_data(self, test_neg_number, data='test'):\n",
    "        size = test_neg_number + 1\n",
    "        batch = np.zeros((size, 2), dtype=np.uint32)\n",
    "\n",
    "        idx = 0\n",
    "        if(data=='test'):\n",
    "            for user, items in self.user_test_item.items():\n",
    "                for item in items:\n",
    "                    batch[idx, :] = [user, item]\n",
    "                    idx += 1\n",
    "\n",
    "                yield items[0], batch\n",
    "                idx = 0\n",
    "\n",
    "        elif(data=='vali'):\n",
    "            for user, items in self.user_vali_item.items():\n",
    "                for item in items:\n",
    "                    batch[idx, :] = [user, item]\n",
    "                    idx += 1\n",
    "\n",
    "                yield items[0], batch\n",
    "                idx = 0\n",
    "        else:\n",
    "            print(\"data type error.\")\n",
    "            exit(-1)\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time.time()\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', mean_adj_mat.shape, time.time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "\n",
    "        return  mean_adj_mat\n",
    "    def get_adj_mat_nonorm(self):\n",
    "        # try:\n",
    "        #     t1 = time()\n",
    "        #     adj_mat = sp.load_npz(self.path + '/adj_mat.npz')\n",
    "        #     print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        # except Exception:\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "\n",
    "        rowsum = np.array(adj_mat.sum(1)).flatten()\n",
    "        d_mat_inv = sp.diags(rowsum)\n",
    "\n",
    "        adj_mat = adj_mat+d_mat_inv\n",
    "\n",
    "        adj_mat = adj_mat.tocsr()\n",
    "        sp.save_npz(self.path + '/adj_mat.npz', adj_mat)\n",
    "\n",
    "        return adj_mat\n",
    "    def get_nodesum(self,depth):\n",
    "        adj_mat = self.get_adj_mat_nonorm()\n",
    "        edge_mat = adj_mat.dot(adj_mat)\n",
    "        for i in range(depth-1):\n",
    "            if(i!=0):\n",
    "                edge_mat = edge_mat.dot(adj_mat)\n",
    "            else:\n",
    "                pass\n",
    "        nodesum = edge_mat.sum(1).flatten()\n",
    "        return nodesum\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time.time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users+self.n_items, self.n_users+self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time.time() - t1)\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        mean_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time.time() - t2)\n",
    "        return mean_adj_mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def leave_one_out(purchased_item, recommend_list, top_k_recommand_number):\n",
    "    top_recommend_list=recommend_list[:top_k_recommand_number]\n",
    "    if (purchased_item in top_recommend_list):\n",
    "        return 1, np.log2(2.0) / np.log2(top_recommend_list.index(purchased_item) + 2.0)\n",
    "    else:\n",
    "        return 0, 0\n",
    "def NDCG_k(recommend_list, purchased_list):\n",
    "    Z_u = 0\n",
    "    temp=0\n",
    "    for j in range(min(len(recommend_list), len(purchased_list))):\n",
    "        Z_u = Z_u + 1 / np.log2(j + 2)\n",
    "    for j in range(len(recommend_list)):\n",
    "        if recommend_list[j] in purchased_list:\n",
    "            temp = temp + 1 / np.log2(j + 2)\n",
    "    if Z_u == 0:\n",
    "        temp = 0\n",
    "    else:\n",
    "        temp = temp / Z_u\n",
    "    return temp\n",
    "def top_k(recommend_list, purchased_list):\n",
    "    temp = []\n",
    "    for j in recommend_list:\n",
    "        if j in purchased_list:\n",
    "            temp.append(j)\n",
    "    if len(temp):\n",
    "        HR = 1\n",
    "    else:\n",
    "        HR = 0\n",
    "    co_length=len(temp)\n",
    "    re_length=len(recommend_list)\n",
    "    pu_length=len(purchased_list)\n",
    "\n",
    "    if re_length == 0:\n",
    "        p = 0.0\n",
    "    else:\n",
    "        p = co_length / float(re_length)\n",
    "\n",
    "    if pu_length == 0:\n",
    "        r = 0.0\n",
    "    else:\n",
    "        r = co_length / float(pu_length)\n",
    "\n",
    "    if r != 0 or p != 0:\n",
    "        f=2.0 * p * r / (p + r)\n",
    "    else:\n",
    "        f=0.0\n",
    "    return p, r, f, HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class params:\n",
    "    test_user_number = 0\n",
    "    neg_number = 1\n",
    "    test_neg_number = 100\n",
    "    learning_rate = 0.0001\n",
    "    batch_size = 1024\n",
    "    pretrain = 10\n",
    "    learner = \"adam\"\n",
    "    n_fold = 1\n",
    "    mess_dropout = 0.0\n",
    "    node_dropout = 0.1\n",
    "    depth = 10\n",
    "    alpha = 0.5\n",
    "    loss = 0\n",
    "    l2_regeularization = 0.0001\n",
    "    number_users = 0\n",
    "    number_items = 0\n",
    "    global_dimention = 50\n",
    "    verbose = 1\n",
    "    note = 'edge-simi'\n",
    "    edge = 'add'\n",
    "    save = 1\n",
    "    outward = 0.5\n",
    "    epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the model\n",
    "\"\"\"\n",
    "class LECF(nn.Cell):\n",
    "    def __init__(self,data):\n",
    "        super(LECF,self).__init__(auto_prefix=True)\n",
    "        # create trainable parameters\n",
    "        self.data = data\n",
    "        self.user_embedding_weight = ms.Parameter(default_input=initializer('XavierUniform',[params.number_users,params.global_dimention] ,ms.float32),name=\"item_embedding_matrix\",requires_grad=True,layerwise_parallel=False)\n",
    "        self.item_embedding_weight = ms.Parameter(default_input=initializer('XavierUniform',[params.number_items,params.global_dimention] ,ms.float32),name=\"item_embedding_matrix\",requires_grad=True,layerwise_parallel=False)\n",
    "        self.edge_weight = ms.Parameter(default_input=initializer('XavierUniform',[2 * params.global_dimention,params.global_dimention] ,ms.float32),name=\"edge_weight\",requires_grad=True,layerwise_parallel=False)\n",
    "        self.dl = 1\n",
    "        if (params.edge == 'concat'): self.dl = 2\n",
    "        self.test_user_g_embeddings = ms.Parameter(default_input=initializer('ones',shape=[params.number_users,params.global_dimention * self.dl] , dtype=ms.float32),name='test_user_g_embeddings',requires_grad=True,layerwise_parallel=False)\n",
    "        self.test_item_g_embeddings = ms.Parameter(default_input=initializer('ones',shape=[params.number_items,params.global_dimention * self.dl] , dtype=ms.float32),name='test_item_g_embeddings',requires_grad=True,layerwise_parallel=False)\n",
    "\n",
    "        # initializing part of the trainable parameters\n",
    "        self.A_fold_hat_c = self._get_fold_hat(params.outward)\n",
    "        self.A_fold_hat_e = self._get_fold_hat(-1)\n",
    "        self.concat0 = P.Concat(axis=0)\n",
    "        self.concat1 = ms.ops.Concat(axis=0)\n",
    "        self.concat2 = P.Concat(axis=0)\n",
    "        self.ego_embeddings = self.concat0((self.user_embedding_weight,self.item_embedding_weight))\n",
    "        print(self.user_embedding_weight.shape,\"::::\", self.item_embedding_weight.shape,\"::::\",self.ego_embeddings.shape)\n",
    "        self.matmul = P.MatMul(transpose_a=False,transpose_b=False)\n",
    "        for K in range(params.depth):\n",
    "            if (K == 0):\n",
    "                self.A_fold_hat = self.A_fold_hat_e\n",
    "            else : \n",
    "                self.A_fold_hat = self.A_fold_hat_c\n",
    "            \n",
    "            temp_embed = []\n",
    "            for G in range(params.n_fold):\n",
    "                # print(\"G is: \",G,\", A_fold_hat is\",self.A_fold_hat[G].shape)\n",
    "                temp_embed.append(self.matmul(Tensor(self.A_fold_hat[G],ms.float32),self.ego_embeddings))\n",
    "            \n",
    "            if (K == 0):\n",
    "                if (params.edge == \"add\"):\n",
    "                    # todo: figure what happened here.\n",
    "                    # self.ego_embeddings += self.concat1(temp_embed)\n",
    "                    self.ego_embeddings += temp_embed[0]\n",
    "                else:\n",
    "                    pass\n",
    "                # todo: other edges.\n",
    "            else: \n",
    "                # self.ego_embeddings = self.concat2((temp_embed,0))\n",
    "                self.ego_embeddings = temp_embed[0]\n",
    "            self.dropout = nn.Dropout(keep_prob=1-params.mess_dropout)\n",
    "\n",
    "            if (params.mess_dropout != 0):\n",
    "                self.ego_embeddings = self.dropout(self.ego_embeddings)\n",
    "        # the network logic\n",
    "        self.l2_normalize = P.L2Normalize(axis=0,epsilon=1e-4)\n",
    "        self.split = P.Split(axis=0,output_num=2)\n",
    "        self.embedding_lookup_FUE = nn.EmbeddingLookup(vocab_size=self.user_embedding_weight.shape[0], \\\n",
    "                        embedding_size=self.user_embedding_weight.shape[1] , param_init=self.user_embedding_weight,target=\"CPU\",slice_mode=\"batch_slice\")\n",
    "        self.embedding_lookup_FIE = nn.EmbeddingLookup(vocab_size=self.item_embedding_weight.shape[0], \\\n",
    "                        embedding_size=self.item_embedding_weight.shape[1] , param_init=self.item_embedding_weight,target=\"CPU\",slice_mode=\"batch_slice\")\n",
    "        self.embedding_lookup_FNIE = nn.EmbeddingLookup(vocab_size=self.item_embedding_weight.shape[0], \\\n",
    "                        embedding_size=self.item_embedding_weight.shape[1] , param_init=self.item_embedding_weight,target=\"CPU\",slice_mode=\"batch_slice\")\n",
    "\n",
    "        self.reduce_sum_y = P.ReduceSum(keep_dims=True)\n",
    "        self.reduce_sum_neg_y = P.ReduceSum(keep_dims=True)\n",
    "        self.matmul_y = P.Mul()\n",
    "        self.matmul_neg_y = P.Mul()\n",
    "\n",
    "\n",
    "    def construct(self, ID):\n",
    "        \"\"\"\n",
    "        the forward pass route.\n",
    "        :param ID: input batch data.\n",
    "        :return: the solution.\n",
    "        \"\"\"\n",
    "        user_id = ID[:,0]\n",
    "        item_id = ID[:,1]\n",
    "        neg_item_id = ID[:,2]\n",
    "        # print(\"user_id is: \",ID[:,0].shape,\" item_id is: \",ID[:,1].shape,\" neg_item_id is: \",ID[:,2].shape)\n",
    "\n",
    "        # norm_embeddings = self.l2_normalize(ego_embeddings)\n",
    "        # users_g_embeddings, items_g_embeddings = self.split(self.ego_embeddings, [int(self.data.n_users), int(self.data.n_items)])\n",
    "\n",
    "        users_g_embeddings = self.ego_embeddings[:int(self.data.n_users),:]\n",
    "        items_g_embeddings = self.ego_embeddings[int(self.data.n_users):,:]\n",
    "        # print(self.data.n_items, self.data.n_users)\n",
    "        # print(\"the shape of g_e is:\", self.ego_embeddings.shape,\" u_g_e is: \",users_g_embeddings.shape,\" i_g_e is: \",items_g_embeddings.shape)\n",
    "        first_user_embedding = self.embedding_lookup_FUE(ms.ops.Cast()(user_id,ms.int32))\n",
    "        # print('the shape of first_usr_embedding is: ',first_user_embedding.shape,\" user_id's shape is: \",user_id.shape)\n",
    "        first_item_embedding = self.embedding_lookup_FIE(ms.ops.Cast()(item_id,ms.int32))\n",
    "        first_neg_item_embedding = self.embedding_lookup_FNIE(ms.ops.Cast()(neg_item_id,ms.int32))\n",
    "\n",
    "        self.embedding_lookup_LUE = nn.EmbeddingLookup(vocab_size=users_g_embeddings.shape[0], \\\n",
    "                        embedding_size=users_g_embeddings.shape[1] , param_init=users_g_embeddings,target=\"CPU\",slice_mode=\"batch_slice\")\n",
    "        self.embedding_lookup_LIE = nn.EmbeddingLookup(vocab_size=items_g_embeddings.shape[0], \\\n",
    "                        embedding_size=items_g_embeddings.shape[1] , param_init=items_g_embeddings,target=\"CPU\",slice_mode=\"batch_slice\")\n",
    "        self.embedding_lookup_LNIE = nn.EmbeddingLookup(vocab_size=items_g_embeddings.shape[0], \\\n",
    "                        embedding_size=items_g_embeddings.shape[1] , param_init=items_g_embeddings,target=\"CPU\",slice_mode=\"batch_slice\")\n",
    "\n",
    "        last_user_embedding = self.embedding_lookup_LUE(ms.ops.Cast()(user_id,ms.int32))\n",
    "        last_item_embedding = self.embedding_lookup_LIE(ms.ops.Cast()(item_id,ms.int32))\n",
    "        last_neg_item_embedding = self.embedding_lookup_LNIE(ms.ops.Cast()(neg_item_id,ms.int32))\n",
    "\n",
    "        query_pair = first_user_embedding + first_item_embedding\n",
    "        neg_query_pair = first_user_embedding + first_neg_item_embedding\n",
    "\n",
    "        # if(params.y == 'edge'):\n",
    "        y = self.reduce_sum_y(self.matmul_y(query_pair,last_user_embedding+last_item_embedding),(1,))\n",
    "\n",
    "        neg_y = self.reduce_sum_neg_y(self.matmul_neg_y(neg_query_pair,last_user_embedding+last_neg_item_embedding),(1,))\n",
    "\n",
    "        return y, neg_y, first_user_embedding, first_item_embedding,first_neg_item_embedding\n",
    "        # return 0 # for test\n",
    "    def _get_fold_hat(self, outward):\n",
    "        mean_adj_mat = self.data.get_adj_mat()\n",
    "        A_fold_hat = []\n",
    "        if(outward==-1):\n",
    "            mat = 0.5*mean_adj_mat\n",
    "        else:\n",
    "            mat= outward*mean_adj_mat + (1-outward)*sp.eye(mean_adj_mat.shape[0])\n",
    "        fold_len = (self.data.n_users + self.data.n_items) // params.n_fold # // operation is rounding down div.\n",
    "        for i_fold in range(params.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if (i_fold == params.n_fold - 1):\n",
    "                end = self.data.n_users + self.data.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "            temp = mat[start:end].todense().astype(np.float32)\n",
    "\n",
    "            \"\"\"\n",
    "            # use the coordinate matrix, which is not supported on CPU.\n",
    "            coo = mat[start:end].tocoo().astype(np.float32)\n",
    "            indices = np.mat([coo.row, coo.col]).transpose()\n",
    "            temp = ms.SparseTensor(indices=indices, values=coo.data, dense_shape=coo.shape)\n",
    "            print(temp.values,temp.indices,temp.dense_shape)\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\"\n",
    "            # ALL the CRUCIAL ops are not implemented in mindspore cpu.\n",
    "            # todo: finish after developed.\n",
    "            if (params.node_dropout != 0):\n",
    "                random_tensor = 1 - params.node_dropout\n",
    "                # original tf ops: random_tensor += tf.random_uniform([mat[start:end].count_nonzero()])\n",
    "                # ms-Ascend ops(not supported on windows): random_tensor += ms.ops.uniform(shape=(mat[start:end].count_nonzero(),)\\\n",
    "                #       ,minval=Tensor(0.0,dtype=ms.float32),maxval=Tensor(1.0,dtype=ms.float32),seed=365,dtype=ms.float32)\n",
    "\n",
    "                # original tf ops: dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "                # ms-Ascend ops(not supported on windows): dropout_mask = ms.ops.cast(ms.ops.Floor(random_tensor), dtype=ms.bool_)\n",
    "\n",
    "                # temp = tf.sparse_retain(temp, dropout_mask) * tf.div(1., 1 - self.node_dropout)\n",
    "                # not implemented on mindspore.\n",
    "            \"\"\"\n",
    "\n",
    "            A_fold_hat.append(temp)\n",
    "        return A_fold_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372 7957 20437 0.001872033755781348\n",
      "the test_user_number is  1372\n",
      "the train_number is  20437\n",
      "<class 'int'> should\n",
      "the user number is:  1372 \n",
      "the item number is:  7957\n"
     ]
    }
   ],
   "source": [
    "# the logic of reading the dataset.\n",
    "\n",
    "dataset_dir = \"./data/video10/video10.npz\"\n",
    "data = Data(npzpath=dataset_dir)\n",
    "params.test_user_number = len(list(data.user_test_item.keys()))\n",
    "print(\"the test_user_number is \",params.test_user_number)\n",
    "params.train_number = data.train_number * params.neg_number\n",
    "print(\"the train_number is \", params.train_number)\n",
    "params.number_users = int(data.n_users)\n",
    "params.number_items = int(data.n_items)\n",
    "print(type(params.number_items),\"should\")\n",
    "print(\"the user number is: \",params.number_users,\"\\nthe item number is: \",params.number_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the matrix is in size:  (100, 100)  and value: Parameter (name=embedding_matrix)\n"
     ]
    }
   ],
   "source": [
    "# part of testing \"initializer\" of Parameter\n",
    "import mindspore as ms\n",
    "from mindspore import Parameter\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore.common.initializer import XavierUniform\n",
    "\n",
    "context.set_context(mode=context.PYNATIVE_MODE,device_target=\"CPU\")\n",
    "init_a_embedding_value = ms.Parameter(default_input=initializer(init=\"XavierUniform\", shape=[100, 100], dtype=ms.int32),name=\"embedding_matrix\",requires_grad=True, layerwise_parallel=False)\n",
    "print(\"the matrix is in size: \",init_a_embedding_value.shape,\" and value:\",init_a_embedding_value)\n",
    "# the shape parameter should be python int, numpy.int32 will lead to error.\n",
    "# tramsform the data could simply use int(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%% part finished\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (9329, 9329) 0.005982160568237305\n",
      "already load adj matrix (9329, 9329) 0.003988981246948242\n",
      "(1372, 50) :::: (7957, 50) :::: (9329, 50)\n",
      "the embeddings include:  [Parameter (name=item_embedding_matrix), Parameter (name=item_embedding_matrix), Parameter (name=edge_weight), Parameter (name=embedding_lookup_FUE.embedding_table), Parameter (name=embedding_lookup_FIE.embedding_table), Parameter (name=embedding_lookup_FNIE.embedding_table), Parameter (name=embedding_lookup_LUE.embedding_table), Parameter (name=embedding_lookup_LIE.embedding_table), Parameter (name=embedding_lookup_LNIE.embedding_table)]\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (1024, 1)\n",
      "the output shape of the net is:  (981, 1)\n"
     ]
    }
   ],
   "source": [
    "context.set_context(mode=context.PYNATIVE_MODE,device_target=\"CPU\",save_graphs=False)\n",
    "net = LECF(data=data)\n",
    "print(\"the embeddings include: \",net.trainable_params())\n",
    "progress = enumerate(data.gen_batch_train_data(params.neg_number, params.batch_size))\n",
    "for k, e in progress:\n",
    "    # LECF.set_train(mode=True)\n",
    "    output = net(Tensor(e,ms.float32))\n",
    "    print(\"the output shape of the net is: \", output[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class with_loss_cell(nn.Cell):\n",
    "    def __init__(self, network):\n",
    "        super(with_loss_cell,self).__init__(auto_prefix=True)\n",
    "        self.network = network\n",
    "        # initialize LOSS\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss_fn_first_item = nn.Norm(axis=0,keep_dims=False)# use nn.Norm as a loss function.\n",
    "        self.loss_fn_first_user = nn.Norm(axis=0,keep_dims=False)\n",
    "        self.loss_fn_first_neg_item_embedding = nn.Norm(axis=0,keep_dims=False)\n",
    "        self.log = P.Log()\n",
    "        self.div = P.RealDiv()\n",
    "        self.add = P.TensorAdd()\n",
    "\n",
    "    def construct(self, x):\n",
    "        y, neg_y, first_user_embedding, first_item_embedding,first_neg_item_embedding = self.network(x)\n",
    "        mf_loss = self.reduce_sum(self.log(self.sigmoid(y - neg_y) + 1e-6))\n",
    "        first_user_loss = self.loss_fn_first_user(first_user_embedding)\n",
    "        first_item_loss = self.loss_fn_first_item(first_item_embedding)\n",
    "        first_neg_item_embedding_loss = self.loss_fn_first_neg_item_embedding(first_neg_item_embedding)\n",
    "        reg_loss = params.l2_regeularization * (first_neg_item_embedding_loss + first_item_loss +\n",
    "            first_user_loss) / params.batch_size\n",
    "        return mf_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class train_one_step_cell(nn.Cell):\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(train_one_step_cell, self).__init__(auto_prefix=True)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.network.add_flags(defer_inline=True)\n",
    "        self.weights = ParameterTuple(network.trainable_params())\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = C.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "\n",
    "    def construct(self,x):\n",
    "        weights = self.weights\n",
    "        loss = self.network(x)\n",
    "        sens = P.Fill()(P.DType()(loss), P.Shape()(loss), self.sens)\n",
    "        grads = self.grad(self.network, weights)(x,sens)\n",
    "        return F.depend(loss, self.optimizer(grads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (9329, 9329) 0.0029959678649902344\n",
      "already load adj matrix (9329, 9329) 0.003988742828369141\n",
      "(1372, 50) :::: (7957, 50) :::: (9329, 50)\n",
      "the original net's embeddings include:  [Parameter (name=network.user_embedding_weight), Parameter (name=network.item_embedding_weight), Parameter (name=network.edge_weight), Parameter (name=network.embedding_lookup_FUE.embedding_table), Parameter (name=network.embedding_lookup_FIE.embedding_table), Parameter (name=network.embedding_lookup_FNIE.embedding_table), Parameter (name=network.embedding_lookup_LUE.embedding_table), Parameter (name=network.embedding_lookup_LIE.embedding_table), Parameter (name=network.embedding_lookup_LNIE.embedding_table)]\n",
      "all embeddings include:  [Parameter (name=network.user_embedding_weight), Parameter (name=network.item_embedding_weight), Parameter (name=network.edge_weight), Parameter (name=network.embedding_lookup_FUE.embedding_table), Parameter (name=network.embedding_lookup_FIE.embedding_table), Parameter (name=network.embedding_lookup_FNIE.embedding_table), Parameter (name=network.embedding_lookup_LUE.embedding_table), Parameter (name=network.embedding_lookup_LIE.embedding_table), Parameter (name=network.embedding_lookup_LNIE.embedding_table)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11224:5820,MainProcess):2021-02-26-00:36:59.672.213 [mindspore\\ops\\operations\\math_ops.py:171] WARN_DEPRECATED: The usage of TensorAdd is deprecated. Please use Add.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mindspore\\ccsrc\\runtime\\device\\cpu\\kernel_select_cpu.cc:299 SetKernelInfo] Operator[MakeRowTensor] is not support. Trace: \nIn file C:\\Users\\15561\\.conda\\envs\\MS\\lib\\site-packages\\mindspore\\ops\\_grad\\grad_array_ops.py(271)/        return RowTensor(new_indices, actual_dout, x_shp), zeros_like(indices), zeros_like(offset)/\n\n\n# ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-101-81b8af71ce82>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[0mnet_train_one_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_train\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mprogress\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnet_train_one_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m     \u001B[1;31m# loss = net_with_loss(Tensor(e,ms.float32))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"the loss of the net is: \"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\" in step: \"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\15561\\.conda\\envs\\MS\\lib\\site-packages\\mindspore\\nn\\cell.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    349\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    350\u001B[0m             \u001B[0m_pynative_exec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menter_construct\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 351\u001B[1;33m             \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconstruct\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mcast_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    352\u001B[0m             \u001B[0m_pynative_exec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mleave_construct\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    353\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mParameter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-100-c3dd164577fc>\u001B[0m in \u001B[0;36mconstruct\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnetwork\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0msens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mP\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFill\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDType\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mP\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mShape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msens\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m         \u001B[0mgrads\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnetwork\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0msens\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdepend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrads\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\15561\\.conda\\envs\\MS\\lib\\site-packages\\mindspore\\common\\api.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*arg, **kwargs)\u001B[0m\n\u001B[0;32m     73\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mwraps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m         \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0m_convert_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\15561\\.conda\\envs\\MS\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\u001B[0m in \u001B[0;36mafter_grad\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    363\u001B[0m                         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Another grad step is running\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    364\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pynative_forward_run\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 365\u001B[1;33m                     \u001B[0m_pynative_exec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    366\u001B[0m                     \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_pynative_exec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    367\u001B[0m                     \u001B[0m_pynative_exec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\15561\\.conda\\envs\\MS\\lib\\site-packages\\mindspore\\common\\api.py\u001B[0m in \u001B[0;36mgrad\u001B[1;34m(self, grad, obj, weights, *args, **kwargs)\u001B[0m\n\u001B[0;32m    319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    320\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 321\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_executor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad_net\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    322\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    323\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcell_id\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mindspore\\ccsrc\\runtime\\device\\cpu\\kernel_select_cpu.cc:299 SetKernelInfo] Operator[MakeRowTensor] is not support. Trace: \nIn file C:\\Users\\15561\\.conda\\envs\\MS\\lib\\site-packages\\mindspore\\ops\\_grad\\grad_array_ops.py(271)/        return RowTensor(new_indices, actual_dout, x_shp), zeros_like(indices), zeros_like(offset)/\n\n\n# "
     ]
    }
   ],
   "source": [
    "context.set_context(mode=context.PYNATIVE_MODE,device_target=\"CPU\",save_graphs=False)\n",
    "net = LECF(data=data)\n",
    "loss = nn.Norm(axis=0,keep_dims=False)\n",
    "net_with_loss = with_loss_cell(network=net)\n",
    "# ms.nn.TrainOneStepCell\n",
    "opt = nn.Adam(net.trainable_params())\n",
    "net_train_one_step = train_one_step_cell(network=net_with_loss, optimizer=opt)\n",
    "net_train_one_step.requires_grad = True\n",
    "print(\"the original net's embeddings include: \",net.trainable_params())\n",
    "print(\"all embeddings include: \",net_with_loss.trainable_params())\n",
    "progress = enumerate(data.gen_batch_train_data(params.neg_number, params.batch_size))\n",
    "net_train_one_step.set_train(mode=True)\n",
    "for k, e in progress:\n",
    "    # loss = net_train_one_step(Tensor(e,ms.float32))\n",
    "    loss = net_with_loss(Tensor(e,ms.float32))\n",
    "    print(\"the loss of the net is: \",loss,\" in step: \",k)\n",
    "    exit(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-MS] *",
   "language": "python",
   "name": "conda-env-.conda-MS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}