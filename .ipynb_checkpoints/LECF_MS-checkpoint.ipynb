{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(13656:9788,MainProcess):2021-02-22-15:27:12.509.429 [mindspore\\ops\\operations\\array_ops.py:2302] WARN_DEPRECATED: The usage of Pack is deprecated. Please use Stack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 'ControlDepend' is deprecated from version 1.1 and will be removed in a future version, use 'Depend' instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore import nn\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore.common.parameter import ParameterTuple\n",
    "from mindspore.ops import composite as C\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore import Tensor\n",
    "from mindspore.nn.layer.activation import get_activation\n",
    "import mindspore.context as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load data\n",
    "\"\"\"\n",
    "class Data(object):\n",
    "    def __init__(self,npzpath=\"./data/viedo10/video10.npz\"):\n",
    "\n",
    "        self.user_item = defaultdict(set)\n",
    "        self.item_user = defaultdict(set)\n",
    "\n",
    "        self.user_vali_item = dict()\n",
    "        self.user_test_item = dict()\n",
    "\n",
    "        _data = np.load(npzpath, allow_pickle=True)\n",
    "        self.train_data = _data['train_data']\n",
    "        self.test_data = _data['test_data'].tolist()\n",
    "        vali_data = _data['vali_data'].tolist()\n",
    "\n",
    "        # todo consider using os.path.join\n",
    "        p = npzpath.split('/')\n",
    "        self.path = p[0] + '/' + p[1] + '/' + p[2]\n",
    "\n",
    "        self.n_users, self.n_items = self.train_data.max(axis=0) + 1\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        for u, i in self.train_data:\n",
    "            self.user_item[u].add(i)\n",
    "            self.item_user[i].add(u)\n",
    "\n",
    "            self.R[u, i] = 1.\n",
    "\n",
    "        self.train_number = np.shape(self.train_data)[0]\n",
    "        print(self.n_users, self.n_items,self.train_number, self.train_number/(self.n_users*self.n_items))\n",
    "\n",
    "        for u in self.test_data.keys():\n",
    "            self.user_test_item[u]=[self.test_data[u][0]]\n",
    "            self.user_test_item[u].extend(self.test_data[u][1])\n",
    "\n",
    "        for u in vali_data.keys():\n",
    "            self.user_vali_item[u] = [vali_data[u][0]]\n",
    "            self.user_vali_item[u].extend(vali_data[u][1])\n",
    "\n",
    "        # self.nodesum = self.get_nodesum(depth)\n",
    "\n",
    "\n",
    "    def gen_batch_train_data(self, neg_number, batch_size):\n",
    "        np.random.shuffle(self.train_data)\n",
    "        batch = np.zeros((batch_size, 3), dtype=np.uint32)\n",
    "        idx = 0\n",
    "        for u,i in self.train_data:\n",
    "            for neg_num in range(neg_number):\n",
    "                neg_item = random.randint(0, self.n_items - 1)\n",
    "                while (neg_item in self.user_item[u]):\n",
    "                    neg_item = random.randint(0, self.n_items  - 1)\n",
    "                batch[idx, :] = [u,i, neg_item]\n",
    "                idx += 1\n",
    "\n",
    "                if (idx == batch_size):\n",
    "                    yield batch\n",
    "                    idx = 0\n",
    "\n",
    "        if (idx > 0):\n",
    "            yield batch[:idx]\n",
    "\n",
    "\n",
    "    def gen_batch_test_data(self, test_neg_number, data='test'):\n",
    "        size = test_neg_number + 1\n",
    "        batch = np.zeros((size, 2), dtype=np.uint32)\n",
    "\n",
    "        idx = 0\n",
    "        if(data=='test'):\n",
    "            for user, items in self.user_test_item.items():\n",
    "                for item in items:\n",
    "                    batch[idx, :] = [user, item]\n",
    "                    idx += 1\n",
    "\n",
    "                yield items[0], batch\n",
    "                idx = 0\n",
    "\n",
    "        elif(data=='vali'):\n",
    "            for user, items in self.user_vali_item.items():\n",
    "                for item in items:\n",
    "                    batch[idx, :] = [user, item]\n",
    "                    idx += 1\n",
    "\n",
    "                yield items[0], batch\n",
    "                idx = 0\n",
    "        else:\n",
    "            print(\"data type error.\")\n",
    "            exit(-1)\n",
    "\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', mean_adj_mat.shape, time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "\n",
    "        return  mean_adj_mat\n",
    "\n",
    "    def get_adj_mat_nonorm(self):\n",
    "        # try:\n",
    "        #     t1 = time()\n",
    "        #     adj_mat = sp.load_npz(self.path + '/adj_mat.npz')\n",
    "        #     print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        # except Exception:\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "\n",
    "        rowsum = np.array(adj_mat.sum(1)).flatten()\n",
    "        d_mat_inv = sp.diags(rowsum)\n",
    "\n",
    "        adj_mat = adj_mat+d_mat_inv\n",
    "\n",
    "        adj_mat = adj_mat.tocsr()\n",
    "        sp.save_npz(self.path + '/adj_mat.npz', adj_mat)\n",
    "\n",
    "        return adj_mat\n",
    "\n",
    "    def get_nodesum(self,depth):\n",
    "        adj_mat = self.get_adj_mat_nonorm()\n",
    "        edge_mat = adj_mat.dot(adj_mat)\n",
    "        for i in range(depth-1):\n",
    "            if(i!=0):\n",
    "                edge_mat = edge_mat.dot(adj_mat)\n",
    "            else:\n",
    "                pass\n",
    "        nodesum = edge_mat.sum(1).flatten()\n",
    "        return nodesum\n",
    "\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users+self.n_items, self.n_users+self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        mean_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time() - t2)\n",
    "        return mean_adj_mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def leave_one_out(purchased_item, recommend_list, top_k_recommand_number):\n",
    "    top_recommend_list=recommend_list[:top_k_recommand_number]\n",
    "    if (purchased_item in top_recommend_list):\n",
    "        return 1, np.log2(2.0) / np.log2(top_recommend_list.index(purchased_item) + 2.0)\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "def NDCG_k(recommend_list, purchased_list):\n",
    "    Z_u = 0\n",
    "    temp=0\n",
    "    for j in range(min(len(recommend_list), len(purchased_list))):\n",
    "        Z_u = Z_u + 1 / np.log2(j + 2)\n",
    "    for j in range(len(recommend_list)):\n",
    "        if recommend_list[j] in purchased_list:\n",
    "            temp = temp + 1 / np.log2(j + 2)\n",
    "    if Z_u == 0:\n",
    "        temp = 0\n",
    "    else:\n",
    "        temp = temp / Z_u\n",
    "    return temp\n",
    "\n",
    "def top_k(recommend_list, purchased_list):\n",
    "    temp = []\n",
    "    for j in recommend_list:\n",
    "        if j in purchased_list:\n",
    "            temp.append(j)\n",
    "    if len(temp):\n",
    "        HR = 1\n",
    "    else:\n",
    "        HR = 0\n",
    "    co_length=len(temp)\n",
    "    re_length=len(recommend_list)\n",
    "    pu_length=len(purchased_list)\n",
    "\n",
    "    if re_length == 0:\n",
    "        p = 0.0\n",
    "    else:\n",
    "        p = co_length / float(re_length)\n",
    "\n",
    "    if pu_length == 0:\n",
    "        r = 0.0\n",
    "    else:\n",
    "        r = co_length / float(pu_length)\n",
    "\n",
    "    if r != 0 or p != 0:\n",
    "        f=2.0 * p * r / (p + r)\n",
    "    else:\n",
    "        f=0.0\n",
    "    return p, r, f, HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class params:\n",
    "    test_user_number = 0\n",
    "    neg_number = 1\n",
    "    test_neg_number = 100\n",
    "    learning_rate = 0.0001\n",
    "    batch_size = 1024\n",
    "    pretrain = 10\n",
    "    learner = \"adam\"\n",
    "    n_fold = 1\n",
    "    mess_dropout = 0.0\n",
    "    node_dropout = 0.1\n",
    "    depth = 10\n",
    "    alpha = 0.5\n",
    "    loss = 0\n",
    "    l2_regeularization = 0.0001\n",
    "    number_users = 0\n",
    "    number_items = 0\n",
    "    global_dimention = 50\n",
    "    verbose =1\n",
    "    note = 'edge-simi'\n",
    "    edge = 'add'\n",
    "    save = 1\n",
    "    outward = 0.5\n",
    "    epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-1-074928840ed5>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-074928840ed5>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    self.mf_loss = - self.reduce_sum_in_loss()\u001b[0m\n\u001b[1;37m                                              \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "define the model\n",
    "\"\"\"\n",
    "class LECF(nn.Cell):\n",
    "    def __init__(self,data):\n",
    "        super(LECF,self).__init__()\n",
    "        # initial parameters\n",
    "        self.data = data\n",
    "        self.user_embedding_weight = ms.Parameter(default_input=initializer('XavierUniform',[params.number_users,params.global_dimention] \\\n",
    "            ,ms.int32),name=\"user_embedding_matrix\",requires_grad=True,layerwise_parallel=False)\n",
    "        self.item_embedding_weight = ms.Parameter(default_input=initializer('XavierUniform',[params.number_items,params.global_dimention] \\\n",
    "            ,ms.int32),name=\"item_embedding_matrix\",requires_grad=True,layerwise_parallel=False)\n",
    "        self.edge_weight = ms.Parameter(default_input=initializer('XavierUniform',[2 * params.global_dimention,params.global_dimention] \\\n",
    "            ,ms.int32),name=\"edge_weight\",requires_grad=True,layerwise_parallel=False)\n",
    "        self.dl = 1\n",
    "        if (params.edge == 'concat'): self.dl = 2\n",
    "        self.test_user_g_embeddings = ms.Parameter(default_input=initializer('ones',shape=[params.number_users,params.global_dimention * self.dl] \\\n",
    "            , dtype=ms.float32),name='test_user_g_embeddings',requires_grad=True,layerwise_parallel=False)\n",
    "        self.test_item_g_embeddings = ms.Parameter(default_input=initializer('ones',shape=[params.number_items,params.global_dimention * self.dl] \\\n",
    "            , dtype=ms.float32),name='test_item_g_embeddings',requires_grad=True,layerwise_parallel=False)\n",
    "        self.node_dropout = Tensor(0,ms.float32)\n",
    "        self.mess_dropout = Tensor(0,ms.float32)\n",
    "        self.user_id = Tensor(0,ms.int32)\n",
    "        self.item_id = Tensor(0,ms.int32)\n",
    "        self.neg_item_id = Tensor(0,ms.int32)\n",
    "        \n",
    "        # build the model (front pass)\n",
    "        self.A_fold_hat_c = self.__get_fold_hat__(params.outward)\n",
    "        self.A_fold_hat_e = self.__get_fold_hat__(-1)\n",
    "        self.concat0 = P.Concat(axis=0)\n",
    "        self.concat1 = P.Concat(axis=0)\n",
    "        self.concat2 = P.Concat(axis=0)\n",
    "        self.ego_embeddings = self.concat0(self.user_embedding_weight,self.item_embedding_weight)\n",
    "        self.matmul = P.MatMul(transpose_a=False,transpose_b=False)\n",
    "        for K in range(params.depth):\n",
    "            if (K == 0):\n",
    "                self.A_fold_hat = self.A_fold_hat_e\n",
    "            else : \n",
    "                self.A_fold_hat = self.A_fold_hat_c\n",
    "            \n",
    "            temp_embed = []\n",
    "            for G in range(params.n_fold):\n",
    "                temp_embed.append(self.matmul(self.A_fold_hat[G],self.ego_embeddings))\n",
    "            \n",
    "            if (K == 0):\n",
    "                if (params.edge == \"add\"):\n",
    "                    self.ego_embeddings += self.concat1(temp_embed,0)\n",
    "                else:\n",
    "                    pass\n",
    "                # todo\n",
    "            else: \n",
    "                self.ego_embeddings = self.concat2(temp_embed,0)\n",
    "            self.dropout = nn.Dropout(keep_prob=1-self.mess_dropout)\n",
    "            if (params.mess_dropout != 0):\n",
    "                self.ego_embeddings = self.dropout(self.ego_embeddings)\n",
    "\n",
    "        # initialize LOSS\n",
    "        self.reduce_sum_in_loss = P.ReduceSum()\n",
    "        self.sigmoid_in_loss = nn.Sigmoid()\n",
    "###############todo : find another loss fcn.\n",
    "        self.loss_first_item = P.L2Loss()\n",
    "        self.loss_first_user = P.L2Loss()\n",
    "        self.loss_first_neg_item_embedding = P.L2Loss()\n",
    "        self.log = P.Log()\n",
    "        \"\"\"\n",
    "        # may be used in creat_loss\n",
    "        self.div = P.RealDiv()\n",
    "        self.add = P.TensorAdd()\n",
    "        \"\"\"\n",
    "    \n",
    "    def __get_fold_hat__(self, outward):\n",
    "        mean_adj_mat = self.data.get_adj_mat()\n",
    "\n",
    "        A_fold_hat = []\n",
    "\n",
    "        if(outward==-1):\n",
    "            mat = 0.5*mean_adj_mat\n",
    "        else:\n",
    "            mat= outward*mean_adj_mat + (1-outward)*sp.eye(mean_adj_mat.shape[0])\n",
    "\n",
    "        fold_len = (self.data.n_users + self.data.n_items) // params.n_fold\n",
    "        for i_fold in range(params.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if (i_fold == self.para.n_fold - 1):\n",
    "                end = self.data.n_users + self.data.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "############## todo : figure what is happening here.\n",
    "            coo = mat[start:end].tocoo().astype(np.float32)\n",
    "            indices = np.mat([coo.row, coo.col]).transpose()\n",
    "            temp = tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "            if (self.para.node_dropout != 0):\n",
    "                random_tensor = 1 - self.node_dropout\n",
    "                random_tensor += tf.random_uniform([mat[start:end].count_nonzero()])\n",
    "                dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "                temp = tf.sparse_retain(temp, dropout_mask) * tf.div(1., 1 - self.node_dropout)\n",
    "\n",
    "            A_fold_hat.append(temp)\n",
    "        return A_fold_hat\n",
    "    \n",
    "    def construct(self, ID):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param inputs: \n",
    "        :param kwargs: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        self.user_id = ID[0]\n",
    "        self.item_id = ID[1]\n",
    "        self.neg_item_id = ID[2]\n",
    "\n",
    "        self.first_user_embedding = []\n",
    "        for i in range(self.user_id[i]):\n",
    "            self.first_user_embedding.append(self.user_embedding_weight[i])\n",
    "        \"\"\"\n",
    "        # create_loss\n",
    "        self.mf_loss = - self.reduce_sum_in_loss(self.log(self.sigmoid_in_loss(self.y - self.neg_y) + 1e-6))\n",
    "        self.first_user_loss = self.loss_first_user(self.first_user_embedding)\n",
    "        self.first_item_loss = self.loss_first_item(self.first_item_embedding)\n",
    "        self.first_neg_item_embedding_loss = self.loss_first_neg_item_embedding(self.first_neg_item_embedding)\n",
    "        self.reg_loss = params.l2_regeularization * (self.first_neg_item_embedding_loss + self.first_item_loss + \\\n",
    "            self.first_user_loss) / params.batch_size\n",
    "        self.loss = self.mf_loss + self.reg_loss\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"./data/video10/video10.npz\"\n",
    "data = Data(npzpath=dataset_dir)\n",
    "params.test_user_number = len(list(data.user_test_item.keys()))\n",
    "params.train_number = data.train_number * params.neg_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "context.set_context(mode=context.PYNATIVE_MODE,device_target=\"CPU\",save_graphs=False)\n",
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-MS] *",
   "language": "python",
   "name": "conda-env-.conda-MS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
