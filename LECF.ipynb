{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-726d594bf673>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtime\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtqdm\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import mindspore as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load data\n",
    "\"\"\"\n",
    "class Data(object):\n",
    "    def __init__(self,npzpath=\"./data/viedo10/video10.npz\"):\n",
    "\n",
    "        self.user_item = defaultdict(set)\n",
    "        self.item_user = defaultdict(set)\n",
    "\n",
    "        self.user_vali_item = dict()\n",
    "        self.user_test_item = dict()\n",
    "\n",
    "        _data = np.load(npzpath, allow_pickle=True)\n",
    "        self.train_data = _data['train_data']\n",
    "        self.test_data = _data['test_data'].tolist()\n",
    "        vali_data = _data['vali_data'].tolist()\n",
    "\n",
    "        # todo consider using os.path.join\n",
    "        p = npzpath.split('/')\n",
    "        self.path = p[0] + '/' + p[1] + '/' + p[2]\n",
    "\n",
    "        self.n_users, self.n_items = self.train_data.max(axis=0) + 1\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        for u, i in self.train_data:\n",
    "            self.user_item[u].add(i)\n",
    "            self.item_user[i].add(u)\n",
    "\n",
    "            self.R[u, i] = 1.\n",
    "\n",
    "        self.train_number = np.shape(self.train_data)[0]\n",
    "        print(self.n_users, self.n_items,self.train_number, self.train_number/(self.n_users*self.n_items))\n",
    "\n",
    "        for u in self.test_data.keys():\n",
    "            self.user_test_item[u]=[self.test_data[u][0]]\n",
    "            self.user_test_item[u].extend(self.test_data[u][1])\n",
    "\n",
    "        for u in vali_data.keys():\n",
    "            self.user_vali_item[u] = [vali_data[u][0]]\n",
    "            self.user_vali_item[u].extend(vali_data[u][1])\n",
    "\n",
    "        # self.nodesum = self.get_nodesum(depth)\n",
    "\n",
    "\n",
    "    def gen_batch_train_data(self, neg_number, batch_size):\n",
    "        np.random.shuffle(self.train_data)\n",
    "        batch = np.zeros((batch_size, 3), dtype=np.uint32)\n",
    "        idx = 0\n",
    "        for u,i in self.train_data:\n",
    "            for neg_num in range(neg_number):\n",
    "                neg_item = random.randint(0, self.n_items - 1)\n",
    "                while (neg_item in self.user_item[u]):\n",
    "                    neg_item = random.randint(0, self.n_items  - 1)\n",
    "                batch[idx, :] = [u,i, neg_item]\n",
    "                idx += 1\n",
    "\n",
    "                if (idx == batch_size):\n",
    "                    yield batch\n",
    "                    idx = 0\n",
    "\n",
    "        if (idx > 0):\n",
    "            yield batch[:idx]\n",
    "\n",
    "\n",
    "    def gen_batch_test_data(self, test_neg_number, data='test'):\n",
    "        size = test_neg_number + 1\n",
    "        batch = np.zeros((size, 2), dtype=np.uint32)\n",
    "\n",
    "        idx = 0\n",
    "        if(data=='test'):\n",
    "            for user, items in self.user_test_item.items():\n",
    "                for item in items:\n",
    "                    batch[idx, :] = [user, item]\n",
    "                    idx += 1\n",
    "\n",
    "                yield items[0], batch\n",
    "                idx = 0\n",
    "\n",
    "        elif(data=='vali'):\n",
    "            for user, items in self.user_vali_item.items():\n",
    "                for item in items:\n",
    "                    batch[idx, :] = [user, item]\n",
    "                    idx += 1\n",
    "\n",
    "                yield items[0], batch\n",
    "                idx = 0\n",
    "        else:\n",
    "            print(\"data type error.\")\n",
    "            exit(-1)\n",
    "\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', mean_adj_mat.shape, time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "\n",
    "        return  mean_adj_mat\n",
    "\n",
    "    def get_adj_mat_nonorm(self):\n",
    "        # try:\n",
    "        #     t1 = time()\n",
    "        #     adj_mat = sp.load_npz(self.path + '/adj_mat.npz')\n",
    "        #     print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        # except Exception:\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "\n",
    "        rowsum = np.array(adj_mat.sum(1)).flatten()\n",
    "        d_mat_inv = sp.diags(rowsum)\n",
    "\n",
    "        adj_mat = adj_mat+d_mat_inv\n",
    "\n",
    "        adj_mat = adj_mat.tocsr()\n",
    "        sp.save_npz(self.path + '/adj_mat.npz', adj_mat)\n",
    "\n",
    "        return adj_mat\n",
    "\n",
    "    def get_nodesum(self,depth):\n",
    "        adj_mat = self.get_adj_mat_nonorm()\n",
    "        edge_mat = adj_mat.dot(adj_mat)\n",
    "        for i in range(depth-1):\n",
    "            if(i!=0):\n",
    "                edge_mat = edge_mat.dot(adj_mat)\n",
    "            else:\n",
    "                pass\n",
    "        nodesum = edge_mat.sum(1).flatten()\n",
    "        return nodesum\n",
    "\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users+self.n_items, self.n_users+self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        mean_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time() - t2)\n",
    "        return mean_adj_mat.tocsr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def leave_one_out(purchased_item, recommend_list, top_k_recommand_number):\n",
    "    top_recommend_list=recommend_list[:top_k_recommand_number]\n",
    "    if (purchased_item in top_recommend_list):\n",
    "        return 1, np.log2(2.0) / np.log2(top_recommend_list.index(purchased_item) + 2.0)\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "def NDCG_k(recommend_list, purchased_list):\n",
    "    Z_u = 0\n",
    "    temp=0\n",
    "    for j in range(min(len(recommend_list), len(purchased_list))):\n",
    "        Z_u = Z_u + 1 / np.log2(j + 2)\n",
    "    for j in range(len(recommend_list)):\n",
    "        if recommend_list[j] in purchased_list:\n",
    "            temp = temp + 1 / np.log2(j + 2)\n",
    "    if Z_u == 0:\n",
    "        temp = 0\n",
    "    else:\n",
    "        temp = temp / Z_u\n",
    "    return temp\n",
    "\n",
    "def top_k(recommend_list, purchased_list):\n",
    "    temp = []\n",
    "    for j in recommend_list:\n",
    "        if j in purchased_list:\n",
    "            temp.append(j)\n",
    "    if len(temp):\n",
    "        HR = 1\n",
    "    else:\n",
    "        HR = 0\n",
    "    co_length=len(temp)\n",
    "    re_length=len(recommend_list)\n",
    "    pu_length=len(purchased_list)\n",
    "\n",
    "    if re_length == 0:\n",
    "        p = 0.0\n",
    "    else:\n",
    "        p = co_length / float(re_length)\n",
    "\n",
    "    if pu_length == 0:\n",
    "        r = 0.0\n",
    "    else:\n",
    "        r = co_length / float(pu_length)\n",
    "\n",
    "    if r != 0 or p != 0:\n",
    "        f=2.0 * p * r / (p + r)\n",
    "    else:\n",
    "        f=0.0\n",
    "    return p, r, f, HR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the model\n",
    "\"\"\"\n",
    "\n",
    "class LECF():\n",
    "    def __init__(self, args):\n",
    "        self.para = args\n",
    "\n",
    "        self.filename = './data/' + self.para.dataset + '/' + self.para.dataset + '.npz'\n",
    "        self.data = Data(self.filename)\n",
    "        self.test_user_number = len(list(self.data.user_test_item.keys()))\n",
    "        self.train_number = self.data.train_number * self.para.neg_number\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        # self.initializer = tf.random_uniform_initializer(minval=0,maxval=0.1)  # tf.truncated_normal_initializer(stddev=0.1)#\n",
    "        self.all_weights = dict()\n",
    "        self.all_weights['user_embedding'] = tf.get_variable('user_embedding_matrix', initializer=initializer,\n",
    "                                                             shape=[self.data.n_users, self.para.global_dimension])\n",
    "        self.all_weights['item_embedding'] = tf.get_variable('item_embedding_matrix', initializer=initializer,\n",
    "                                                             shape=[self.data.n_items, self.para.global_dimension])\n",
    "\n",
    "        self.all_weights['edge_weight'] = tf.get_variable('edge_weight', initializer=initializer,\n",
    "                                            shape=[2 * self.para.global_dimension, self.para.global_dimension])\n",
    "\n",
    "        self.dl=1\n",
    "        if(self.para.edge=='concat'):\n",
    "            self.dl=2\n",
    "        self.test_u_g_embeddings = tf.get_variable('test_u_g_embeddings',\n",
    "                                                   shape=[self.data.n_users,\n",
    "                                                          self.para.global_dimension*self.dl])\n",
    "        self.test_i_g_embeddings = tf.get_variable('test_i_g_embeddings',\n",
    "                                                   shape=[self.data.n_items,\n",
    "                                                          self.para.global_dimension*self.dl])\n",
    "\n",
    "        self.node_dropout = tf.placeholder(tf.float32)\n",
    "        self.mess_dropout = tf.placeholder(tf.float32)\n",
    "        self.user_id = tf.placeholder(tf.int32, shape=[None], name='user_id')\n",
    "        self.item_id = tf.placeholder(tf.int32, shape=[None], name='item_id')\n",
    "        self.neg_item_id = tf.placeholder(tf.int32, shape=[None], name='neg_item_id')\n",
    "\n",
    "    def get_fold_hat(self, outward):\n",
    "        mean_adj_mat = self.data.get_adj_mat()\n",
    "\n",
    "        A_fold_hat = []\n",
    "\n",
    "        if(outward==-1):\n",
    "            mat = 0.5*mean_adj_mat\n",
    "            # mat = mat.tocsr()\n",
    "        else:\n",
    "            mat= outward*mean_adj_mat + (1-outward)*sp.eye(mean_adj_mat.shape[0])\n",
    "            # mat = mat.tocsr()\n",
    "\n",
    "        fold_len = (self.data.n_users + self.data.n_items) // self.para.n_fold\n",
    "        for i_fold in range(self.para.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if (i_fold == self.para.n_fold - 1):\n",
    "                end = self.data.n_users + self.data.n_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "\n",
    "            coo = mat[start:end].tocoo().astype(np.float32)\n",
    "            indices = np.mat([coo.row, coo.col]).transpose()\n",
    "            temp = tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "            if (self.para.node_dropout != 0):\n",
    "                random_tensor = 1 - self.node_dropout\n",
    "                random_tensor += tf.random_uniform([mat[start:end].count_nonzero()])\n",
    "                dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "                temp = tf.sparse_retain(temp, dropout_mask) * tf.div(1., 1 - self.node_dropout)\n",
    "\n",
    "            A_fold_hat.append(temp)\n",
    "        return A_fold_hat\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        A_fold_hat_c = self.get_fold_hat(self.para.outward)\n",
    "        A_fold_hat_e = self.get_fold_hat(-1)\n",
    "\n",
    "        ego_embeddings = tf.concat([self.all_weights['user_embedding'], self.all_weights['item_embedding']], axis=0)\n",
    "\n",
    "        for k in range(self.para.depth):\n",
    "\n",
    "            if(k==0):\n",
    "                A_fold_hat = A_fold_hat_e\n",
    "            else:\n",
    "                A_fold_hat = A_fold_hat_c\n",
    "\n",
    "\n",
    "            temp_embed = []\n",
    "            for f in range(self.para.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "\n",
    "            if(k==0):\n",
    "                if(self.para.edge=='hadam'):\n",
    "                    ego_embeddings = tf.multiply(tf.concat(temp_embed, 0),ego_embeddings)\n",
    "                elif (self.para.edge == 'weight1'):\n",
    "                    ego_embeddings = tf.matmul(tf.concat([tf.concat(temp_embed, 0), ego_embeddings],1),self.all_weights['edge_weights'])\n",
    "                elif (self.para.edge == 'concat'):\n",
    "                    ego_embeddings = tf.concat([tf.concat(temp_embed, 0), 0.5*ego_embeddings],1)\n",
    "                elif (self.para.edge == 'add'):\n",
    "                    ego_embeddings = tf.concat(temp_embed, 0) + ego_embeddings\n",
    "                elif (self.para.edge == 'subtract'):\n",
    "                    ego_embeddings = ego_embeddings - tf.concat(temp_embed, 0)\n",
    "\n",
    "            else:\n",
    "                ego_embeddings = tf.concat(temp_embed, 0)\n",
    "\n",
    "            if(self.para.mess_dropout != 0):\n",
    "                ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - self.mess_dropout)\n",
    "\n",
    "        # norm_embeddings = tf.nn.l2_normalize(ego_embeddings)\n",
    "        self.u_g_embeddings, self.i_g_embeddings = tf.split(ego_embeddings, [int(self.data.n_users), int(self.data.n_items)], 0)\n",
    "\n",
    "        self.first_user_embedding = tf.nn.embedding_lookup(self.all_weights['user_embedding'], self.user_id)\n",
    "        self.first_item_embedding = tf.nn.embedding_lookup(self.all_weights['item_embedding'], self.item_id)\n",
    "        self.first_neg_item_embedding = tf.nn.embedding_lookup(self.all_weights['item_embedding'], self.neg_item_id)\n",
    "\n",
    "        self.last_user_embedding = tf.nn.embedding_lookup(self.u_g_embeddings, self.user_id)\n",
    "        self.last_item_embedding = tf.nn.embedding_lookup(self.i_g_embeddings, self.item_id)\n",
    "        self.last_neg_item_embedding = tf.nn.embedding_lookup(self.i_g_embeddings, self.neg_item_id)\n",
    "\n",
    "        self.query_pair = self.edge_embed(self.first_user_embedding, self.first_item_embedding)\n",
    "        self.query_neg_pair = self.edge_embed(self.first_user_embedding, self.first_neg_item_embedding)\n",
    "\n",
    "\n",
    "        # if(self.para.y=='edge'):\n",
    "        self.y = tf.reduce_sum(tf.multiply(self.query_pair, self.last_user_embedding+self.last_item_embedding), 1)\n",
    "        self.neg_y = tf.reduce_sum(tf.multiply(self.query_neg_pair, self.last_user_embedding+self.last_neg_item_embedding), 1)\n",
    "\n",
    "\n",
    "        self.all_loss = self.creat_loss()\n",
    "\n",
    "\n",
    "        if (self.para.learner == 'sgd'):\n",
    "            print('------------------------------sgd')\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.para.learning_rate).minimize(self.all_loss)\n",
    "\n",
    "        elif (self.para.learner == 'adag'):\n",
    "            print('------------------------------adag')\n",
    "            self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.para.learning_rate).minimize(self.all_loss)\n",
    "\n",
    "        elif (self.para.learner == 'adam'):\n",
    "            print('------------------------------adam')\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.para.learning_rate).minimize(self.all_loss)\n",
    "\n",
    "\n",
    "        self.test_user_embedding = tf.nn.embedding_lookup(self.test_u_g_embeddings, self.user_id)\n",
    "        self.test_item_embedding = tf.nn.embedding_lookup(self.test_i_g_embeddings, self.item_id)\n",
    "\n",
    "        if (self.para.y == 'edge'):\n",
    "            self.test_y = tf.reduce_sum(tf.multiply(self.query_pair,self.test_user_embedding + self.test_item_embedding), 1)\n",
    "\n",
    "        test_top_value, self.test_top_index = tf.nn.top_k(self.test_y, k=10, sorted=True)\n",
    "\n",
    "        self.saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "\n",
    "    def creat_loss(self):\n",
    "        self.mf_loss = -tf.reduce_sum(tf.log(tf.nn.sigmoid(self.y - self.neg_y)+1e-6))\n",
    "        self.reg_loss = self.para.l2 * (tf.nn.l2_loss(self.first_user_embedding) +\n",
    "                                        tf.nn.l2_loss(self.first_item_embedding) +\n",
    "                                        tf.nn.l2_loss(self.first_neg_item_embedding)) / self.para.batch_size\n",
    "        return self.mf_loss + self.reg_loss\n",
    "\n",
    "    def edge_embed(self, user, item):\n",
    "        if(self.para.edge=='add'):\n",
    "            return user+item\n",
    "\n",
    "        elif(self.para.edge=='hadam'):\n",
    "            return tf.multiply(user,item)\n",
    "\n",
    "        elif(self.para.edge=='weight1'):\n",
    "            return tf.matmul(tf.concat([user,item], 1), self.edge_weights)\n",
    "\n",
    "        elif (self.para.edge == 'concat'):\n",
    "            return tf.concat([user,item], 1)\n",
    "\n",
    "        elif (self.para.edge == 'subtract'):\n",
    "            return user-item\n",
    "\n",
    "    def run_model(self):\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = self.para.gpu\n",
    "        print('user number:', self.data.n_users, '  item number:', self.data.n_items,'  train number:', self.train_number,)\n",
    "\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        best_epoch = 0\n",
    "        best_result=np.array([0,0,0,0])\n",
    "\n",
    "        for epoch in range(self.para.epochs):\n",
    "            batch_loss = 0\n",
    "\n",
    "            # progress = tqdm(enumerate(\n",
    "            #     self.data.gen_batch_train_data(self.para.neg_number, self.para.batch_size)), dynamic_ncols=True,\n",
    "            #     total=(self.train_number // self.para.batch_size))\n",
    "            progress = enumerate(\n",
    "                self.data.gen_batch_train_data(self.para.neg_number, self.para.batch_size))\n",
    "            for k, e in progress:\n",
    "                feed = {self.user_id: e[:, 0], self.item_id: e[:, 1], self.neg_item_id: e[:, 2],\n",
    "                        self.node_dropout:self.para.node_dropout, self.mess_dropout:self.para.mess_dropout}\n",
    "\n",
    "                _, loss = self.sess.run([self.optimizer, self.all_loss], feed_dict=feed)\n",
    "\n",
    "                batch_loss += loss\n",
    "                # progress.set_description(u\"[{}] Loss: {:,.6f} ----- \".format(epoch, loss))\n",
    "\n",
    "            if (epoch + 1) % self.para.verbose == 0:\n",
    "                feed = {self.node_dropout: 0, self.mess_dropout: 0}\n",
    "                user_embed, item_embed = self.sess.run([self.u_g_embeddings, self.i_g_embeddings], feed_dict=feed)\n",
    "                self.sess.run([\n",
    "                    self.test_u_g_embeddings.assign(user_embed),\n",
    "                    self.test_i_g_embeddings.assign(item_embed)])\n",
    "\n",
    "                vali_result,_ = self.test(epoch,'vali')\n",
    "                test_result,_ = self.test(epoch,'test')\n",
    "\n",
    "                # if(np.sum(vali_result)>np.sum(best_result)):\n",
    "                #     best_result=vali_result\n",
    "                #     best_epoch = epoch\n",
    "                #     if(self.para.save==1):\n",
    "                #         self.save(epoch)\n",
    "                # else:\n",
    "                #     if(epoch-best_epoch)>50:\n",
    "                #         exit(0)\n",
    "\n",
    "    def save(self, epoch):\n",
    "        # self.saver.save(self.sess, self.savepath+'/tf_model', global_step=epoch)\n",
    "        user_embed, item_embed = self.sess.run([self.all_weights['user_embedding'], self.all_weights['item_embedding']])\n",
    "        np.savez(self.savepath + '/' +'embedding.npz', user=user_embed, item=item_embed)\n",
    "\n",
    "\n",
    "    def test(self, epoch, data='test'):\n",
    "        result = np.zeros([self.test_user_number, 4])\n",
    "\n",
    "        # progress_test = tqdm(enumerate(\n",
    "        #     self.data.gen_batch_test_data(self.para.test_neg_number,data)),\n",
    "        #     dynamic_ncols=True, total=self.test_user_number)\n",
    "        progress_test = enumerate(\n",
    "            self.data.gen_batch_test_data(self.para.test_neg_number,data))\n",
    "\n",
    "        for k, e in progress_test:\n",
    "            purchased_item, batch = e\n",
    "            batch_user, batch_item = batch[:, 0], batch[:, 1]\n",
    "\n",
    "            recommend_list = self.sess.run(self.test_top_index,\n",
    "                                           feed_dict={self.user_id: batch_user,\n",
    "                                                      self.item_id: batch_item})\n",
    "            recommend_list = list(np.array(batch_item)[recommend_list])\n",
    "\n",
    "            HR5, NDCG5 = leave_one_out(purchased_item, recommend_list, 5)\n",
    "            HR10, NDCG10 = leave_one_out(purchased_item, recommend_list, 10)\n",
    "\n",
    "            result[k] = np.array([HR5, NDCG5, HR10, NDCG10])\n",
    "            # progress_test.set_description(u\"{}: [{}] HR: {} ----- \".format(data,epoch, HR5))\n",
    "        avg = np.mean(result, axis=0)\n",
    "        self.logger.info(avg)\n",
    "\n",
    "        return avg,result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run CML.\")\n",
    "\n",
    "    parser.add_argument('-g', '--gpu', help='set gpu device number 0-3', type=str, default='0')\n",
    "    parser.add_argument('--note', help='model-note', type=str, default='Edge-simi')\n",
    "    parser.add_argument('--logdir', type=str, default='result/edge-edge')\n",
    "    parser.add_argument('--verbose', help='test fre', type=int, default=1)\n",
    "    parser.add_argument('-d', '--global_dimension', help='Embedding Size', type=int, default=50)\n",
    "    parser.add_argument('--epochs', help='Max epoch', type=int, default=500)\n",
    "    parser.add_argument('-n', '--neg_number', help='Negative Samples Count', type=int, default=1)\n",
    "    parser.add_argument('--test_neg_number', type=int, default=100)\n",
    "    parser.add_argument('-lr', '--learning_rate', help='learning_rate', type=float, default=0.001)\n",
    "    parser.add_argument('--l2', help='l2 Regularization', type=float, default=0.0001)\n",
    "    parser.add_argument('--dataset', help='path to file', type=str, default='video10')\n",
    "    parser.add_argument('-b', '--batch_size', help='Batch Size', type=int, default=1024)\n",
    "    parser.add_argument('--pretrain', help='1:pretrain', type=int, default=0)\n",
    "    parser.add_argument('--learner', help='[sgd, adag, adam]', type=str, default='adam')\n",
    "    parser.add_argument('--n_fold', type=int, default=1)\n",
    "    parser.add_argument('--mess_dropout', type=float, default=0.0)\n",
    "    parser.add_argument('--node_dropout', type=float, default=0.1)\n",
    "    parser.add_argument('--depth', type=int, default=10)\n",
    "    parser.add_argument('--alpha', type=float, default=0.5)\n",
    "    parser.add_argument('--loss', type=int, default=0)\n",
    "    parser.add_argument('-y', type=str,help='edge,dot-1,dot-2,u-i-1,u-i-2', default='edge')\n",
    "    parser.add_argument('--outward', type=float,help='outward', default=0.5)\n",
    "    parser.add_argument('--edge', type=str,help='add,hadam,weight1-2', default='add')\n",
    "    parser.add_argument('--save', type=int,help='save the model', default=1)\n",
    "\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_args()\n",
    "model = LECF(args)\n",
    "model.run_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-.conda-MS-py",
   "language": "python",
   "display_name": "Python [conda env:.conda-MS] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}